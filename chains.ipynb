{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting OPENAI_API_KEY \n",
    "import os \n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.0.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.27 (from langchain-openai)\n",
      "  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting openai<2.0.0,>=1.10.0 (from langchain-openai)\n",
      "  Downloading openai-1.13.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tiktoken<1,>=0.5.2 (from langchain-openai)\n",
      "  Downloading tiktoken-0.6.0-cp39-cp39-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (1.33)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-core<0.2.0,>=0.1.27->langchain-openai)\n",
      "  Downloading langsmith-0.1.23-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (2.5.3)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.9.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.5.2->langchain-openai)\n",
      "  Downloading regex-2023.12.25-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/42.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 42.0/42.0 kB 511.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: idna>=2.8 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain-openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.27->langchain-openai) (2.4)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.27->langchain-openai)\n",
      "  Downloading orjson-3.9.15-cp39-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     -------- ------------------------------- 10.2/50.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 50.7/50.7 kB 652.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.27->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.27->langchain-openai) (2.14.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.27->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programming\\llm_apps\\langchain\\venv\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.27->langchain-openai) (2.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\karth\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>4->openai<2.0.0,>=1.10.0->langchain-openai) (0.4.4)\n",
      "Downloading langchain_openai-0.0.8-py3-none-any.whl (32 kB)\n",
      "Downloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n",
      "   ---------------------------------------- 0.0/256.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/256.9 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 61.4/256.9 kB 3.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 112.6/256.9 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 163.8/256.9 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 256.9/256.9 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
      "   ---------------------------------------- 0.0/227.4 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 112.6/227.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 112.6/227.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 194.6/227.4 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 227.4/227.4 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading tiktoken-0.6.0-cp39-cp39-win_amd64.whl (798 kB)\n",
      "   ---------------------------------------- 0.0/798.7 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 163.8/798.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 215.0/798.7 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 245.8/798.7 kB 1.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 286.7/798.7 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 327.7/798.7 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 399.4/798.7 kB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 481.3/798.7 kB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 501.8/798.7 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 532.5/798.7 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 634.9/798.7 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 634.9/798.7 kB 1.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 665.6/798.7 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 696.3/798.7 kB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 747.5/798.7 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 798.7/798.7 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading langsmith-0.1.23-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 0.0/66.6 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 61.4/66.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 66.6/66.6 kB 895.2 kB/s eta 0:00:00\n",
      "Downloading regex-2023.12.25-cp39-cp39-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 163.8/269.5 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.5/269.5 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading orjson-3.9.15-cp39-none-win_amd64.whl (135 kB)\n",
      "   ---------------------------------------- 0.0/135.9 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 81.9/135.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 135.9/135.9 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: regex, orjson, tiktoken, openai, langsmith, langchain-core, langchain-openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.6.1\n",
      "    Uninstalling openai-1.6.1:\n",
      "      Successfully uninstalled openai-1.6.1\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.77\n",
      "    Uninstalling langsmith-0.0.77:\n",
      "      Successfully uninstalled langsmith-0.0.77\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.7\n",
      "    Uninstalling langchain-core-0.1.7:\n",
      "      Successfully uninstalled langchain-core-0.1.7\n",
      "Successfully installed langchain-core-0.1.30 langchain-openai-0.0.8 langsmith-0.1.23 openai-1.13.3 orjson-3.9.15 regex-2023.12.25 tiktoken-0.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.0 requires langsmith<0.1.0,>=0.0.77, but you have langsmith 0.1.23 which is incompatible.\n",
      "langchain-community 0.0.9 requires langsmith<0.1.0,>=0.0.63, but you have langsmith 0.1.23 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-openai "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from langchain_openai import OpenAI \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template string \n",
    "template = \"\"\"\n",
    "Interprete the text and evaluate the text. \n",
    "sentiment: is the text in positive or negative or neutral sentiment?\n",
    "subject: what subject is the text about? Use exactly one word. \n",
    "\n",
    "Format the output as JSON with the following keys: \n",
    "sentiment \n",
    "subject \n",
    "\n",
    "text: {input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm model instance \n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template \n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "\n",
    "#chain \n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': ' I ordered a pizza, It was delicious',\n",
       " 'text': '\\n{\\n  \"sentiment\": \"positive\",\\n  \"subject\": \"food\"\\n}'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input=\" I ordered a pizza, It was delicious\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sequential Chain \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you want to pass the output from one model to a another model. This can be done with different SequentialChains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interpretation: The text expresses a positive sentiment towards the subject of pizza. The score of polarity is closer to 1, indicating a high level of positivity.\n",
      "\n",
      "Output in JSON format:\n",
      "{\n",
      "  \"sentiment\": \"positive\",\n",
      "  \"subject\": \"pizza\",\n",
      "  \"score\": 0.8\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# chain - 1 : feedback chain \n",
    "template_feedback = \"\"\"\n",
    "Interpret the text and evaluate the text. \n",
    "You need to specify the sentiment, subject and score of polarity. \n",
    "sentiment: is the text in positive, negative  or neutral ? \n",
    "subject: what is the subject in the text ? \n",
    "score: if it is positive, how much positive it is ? \n",
    "rules for score: \n",
    "range should be in 0 to 1\n",
    "if it is negative, it should be closer to zero, \n",
    "if it is positive, it should be closer to 1 \n",
    "if it neutral, it should be closer 0.5  \n",
    "\n",
    "Format the output in JSON format with the following keys: \n",
    "sentiment\n",
    "subject \n",
    "score \n",
    "\n",
    "text: {feedback} \n",
    " \"\"\"\n",
    "\n",
    "\n",
    "# prompt template \n",
    "feedback_prompt_template = PromptTemplate.from_template(template_feedback) \n",
    "\n",
    "# chain \n",
    "feedback_chain = LLMChain(llm=llm, prompt=feedback_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain-2 : response chain according to customers feedback\n",
    "\n",
    "response_template = \"\"\" \n",
    "You are a helpful bot that creates a 'thank you' response text. \n",
    "If customer feedback is negative and their experience is unsatisfied, offer them a real world assistant to talk to. \n",
    "You will get a sentiment, subject and score  \n",
    "\n",
    "\n",
    "text: {feedback_output}\n",
    "\"\"\"\n",
    "\n",
    "# prompt template \n",
    "response_prompt_template = PromptTemplate(template=response_template, input_variables=['feedback_output'])\n",
    "\n",
    "# response_chain \n",
    "response_chain = LLMChain(llm=llm, prompt=response_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "sentiment: positive\n",
      "subject: food\n",
      "score: 0.9\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Thank you for your positive feedback about our food! We strive to provide delicious and high-quality meals for our customers. Your satisfaction is our top priority. We appreciate your support and hope to see you again soon. If you have any further comments or suggestions, please do not hesitate to let us know. Have a great day!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'I order a pizza, It was delicious',\n",
       " 'output': '\\nThank you for your positive feedback about our food! We strive to provide delicious and high-quality meals for our customers. Your satisfaction is our top priority. We appreciate your support and hope to see you again soon. If you have any further comments or suggestions, please do not hesitate to let us know. Have a great day!'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining two chains \n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "all_chains = SimpleSequentialChain(chains=[feedback_chain, response_chain], verbose=True)\n",
    "\n",
    "all_chains.invoke(input=\"I order a pizza, It was delicious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Sentiment: Positive\n",
      "Subject: Pizza \n",
      "Score: 0.9\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Thank you for your positive feedback about our pizza! We are glad to hear that you enjoyed it. If you have any other comments or suggestions, please don't hesitate to let us know. We appreciate your support and hope to see you again soon!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'I order a pizza, It was delicious',\n",
       " 'output': \"\\nThank you for your positive feedback about our pizza! We are glad to hear that you enjoyed it. If you have any other comments or suggestions, please don't hesitate to let us know. We appreciate your support and hope to see you again soon!\"}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positive feedback \n",
    "all_chains.invoke(input=\"I order a pizza, It was delicious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "Interpretation: The person did not have a good experience with their panner tikka masala dish.\n",
      "\n",
      "Evaluation: Negative sentiment, subject is food/dish, score is closer to 0.\n",
      "\n",
      "Output: {\"sentiment\": \"Negative\", \"subject\": \"Food/Dish\", \"score\": \"0.2\"}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Response: I'm sorry to hear that you didn't enjoy your panner tikka masala dish. As a helpful bot, I would suggest speaking with a real world assistant who can address any concerns or dissatisfaction you may have. Your feedback is important to us and we want to make sure you have a positive experience. Thank you for your honesty.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'I order a panner tikka masala, It was awful',\n",
       " 'output': \"\\nResponse: I'm sorry to hear that you didn't enjoy your panner tikka masala dish. As a helpful bot, I would suggest speaking with a real world assistant who can address any concerns or dissatisfaction you may have. Your feedback is important to us and we want to make sure you have a positive experience. Thank you for your honesty.\"}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chains.invoke(input=\"I order a panner tikka masala, It was awful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains can be more complex and not all sequential chains will be as simple as passing a single string as an argument and getting a single string as the output for all steps in the chain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex example of Sequential chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain \n",
    "from langchain_openai import OpenAI \n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm model \n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-1: This is an LLMChain to write a review given a dish name and the experience \n",
    "prompt_review = PromptTemplate.from_template(template=\"You ordered {dish_name} and your experience was {experience}. Write a review on it.\")\n",
    "chain_review = LLMChain(llm=llm, prompt=prompt_review, output_key='review') \n",
    "\n",
    "# Chain-2: This is an LLMChain to write a follow-up comment on given the restaurant review \n",
    "prompt_comment = PromptTemplate.from_template(template=\"Given the restaurant review: {review}, write a follow-up comment\")\n",
    "chain_comment = LLMChain(llm=llm, prompt=prompt_comment, output_key='comment')\n",
    "\n",
    "# Chain-3: This is am LLMChain to summarize a review \n",
    "prompt_summary = PromptTemplate.from_template(template=\"Summarise the review in one short sentence: \\n\\n {review}\")\n",
    "chain_summary = LLMChain(llm=llm, prompt=prompt_summary, output_key='summary')\n",
    "\n",
    "# Chain-4: This is an LLMChain to translate the summary into Italian language\n",
    "prompt_translation = PromptTemplate.from_template(template=\"Translate the summary into Italian language: \\n\\n {summary}\")\n",
    "chain_translation = LLMChain(llm=llm, prompt=prompt_translation, output_key='translation')\n",
    "\n",
    "# combining all the chains together\n",
    "overall_chains = SequentialChain(chains=[chain_review, chain_comment, chain_summary, chain_translation], \n",
    "                                 input_variables=['dish_name', 'experience'], \n",
    "                                 output_variables=['review', 'comment', 'summary', 'translation'])\n",
    "\n",
    "# running sequential chain \n",
    "response = overall_chains({'dish_name': 'pizza salami', 'experience': 'It was awful!'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dish_name': 'pizza salami',\n",
       " 'experience': 'It was awful!',\n",
       " 'review': \"\\n\\nI recently ordered a pizza salami from a local pizzeria and unfortunately, my experience was absolutely awful. I was looking forward to a delicious and satisfying meal, but what I received was far from it.\\n\\nFirst of all, the pizza arrived almost an hour later than the estimated delivery time. This was already a bad start as I was starving and had to wait even longer for my food. When I finally opened the box, I was disappointed to see that the pizza was not even cut properly. The slices were uneven and some were even stuck together, making it difficult to separate them.\\n\\nBut the worst part was the taste. The salami on the pizza was extremely greasy and had a strange, almost rancid flavor. It was clear that the salami was not fresh and had probably been sitting out for a while. The cheese was also not melted properly and was clumpy in some areas. The crust was soggy and lacked any crispiness.\\n\\nI took a few bites hoping that maybe it was just a bad slice, but unfortunately, the entire pizza was the same. I couldn't even finish one slice before I had to throw the rest away. It was a complete waste of money and left me feeling unsatisfied and disappointed.\\n\\nI have ordered from this\",\n",
       " 'comment': '\\n\\nAfter my previous disappointing experience, I decided to give this pizzeria another chance and ordered a different pizza. I am happy to say that this time, my experience was much better.\\n\\nThe pizza arrived on time and was cut properly, which was a good start. The toppings were fresh and flavorful, and the cheese was perfectly melted. The crust was also crispy and had a nice texture.\\n\\nI am glad that I gave this pizzeria another chance because this pizza was delicious and satisfying. It was a complete turnaround from my previous experience and I am happy to say that I will be ordering from here again. I appreciate that they took my previous feedback into consideration and improved their quality. Keep up the good work!',\n",
       " 'summary': ' pizzeria before and have always had a good experience, but this time was a major letdown. I hope they can improve their quality control and ensure that their food is fresh and properly prepared in the future.\\n\\nThe review is about a disappointing experience with a pizza salami from a local pizzeria, due to late delivery, improper cutting, and poor taste.',\n",
       " 'translation': '\\n\\nLa recensione riguarda una deludente esperienza con una pizza salame da una pizzeria locale, a causa di una consegna in ritardo, un taglio improprio e un cattivo sapore. Prima di questo, il recensore aveva sempre avuto una buona esperienza con la pizzeria, ma questa volta è stata una grande delusione. Si spera che possano migliorare il loro controllo di qualità e garantire che il cibo sia fresco e preparato correttamente in futuro.'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Router Chain (MultiPromptChain)\n",
    "\n",
    "Instead of chaining multiple chains together, we can also use an LLM to decide which follow up chain is being used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI \n",
    "from langchain.chains.router import MultiRouteChain \n",
    "from langchain.chains import LLMChain  \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser \n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=a89vqgK-Qcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
